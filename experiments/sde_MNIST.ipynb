{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZQtKphLIfQf",
        "outputId": "91b666c3-1320-4cc8-deb4-2ca78f3bd58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Neural_PDE-Final'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 102 (delta 41), reused 91 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (102/102), 1.16 MiB | 3.08 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Tselmuun0624/Neural_PDE-Final.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('./Neural_PDE-Final')\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyMAU64DJC7h",
        "outputId": "73bfd2f7-1873-4a41-864f-03a1e85319f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Neural_PDE-Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDlKH74sI6Qz",
        "outputId": "b1688fe5-bef6-44bf-e9ab-1ef4701959b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experiments  README.md\ttorchsde\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsde"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRH7RUCWJM9r",
        "outputId": "bfe8124d-a4c5-4a83-8343-908fab00bd6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsde\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from torchsde) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from torchsde) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from torchsde) (2.5.1+cu121)\n",
            "Collecting trampoline>=0.1.2 (from torchsde)\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchsde) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->torchsde) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->torchsde) (3.0.2)\n",
            "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: trampoline, torchsde\n",
            "Successfully installed torchsde-0.2.6 trampoline-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### MNIST Dataset Traning Experiment ##############\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import datasets, transforms\n",
        "import torchsde\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "class MNISTSDEModel(torchsde.SDEIto):\n",
        "    def __init__(self, data_dim, hidden_dim=64, noise_type='diagonal'):\n",
        "        super().__init__(noise_type=noise_type)\n",
        "\n",
        "        self.fc1 = nn.Linear(data_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, data_dim)\n",
        "        self.sigma = nn.Parameter(torch.ones(data_dim))\n",
        "\n",
        "    def f(self, t, x):\n",
        "        # Drift function\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        h = torch.relu(self.fc2(h))\n",
        "        return self.fc3(h)\n",
        "\n",
        "    def g(self, t, x):\n",
        "        # Diffusion function\n",
        "        return self.sigma * torch.ones_like(x)\n",
        "\n",
        "    def h(self, t, x):\n",
        "        # Aux function for SDE\n",
        "        return torch.zeros_like(x)\n",
        "\n",
        "    def sample(self, batch_size, x0, t, dt):\n",
        "\n",
        "        num_steps = int(t / dt)\n",
        "        xs = [x0]\n",
        "        for _ in range(num_steps):\n",
        "            t = torch.tensor([0.0, dt])\n",
        "            x = torchsde.sdeint(self, xs[-1], t, dt=dt)[-1]\n",
        "            xs.append(x)\n",
        "        return torch.stack(xs)\n",
        "\n",
        "def train_mnist_sde(epochs=10, batch_size=128, learning_rate=0.001, validation_split=0.5):\n",
        "    # Normalizing data for our model\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Load MNIST Dataset\n",
        "    full_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Split the training and test dataset equally\n",
        "    train_size = int(len(full_dataset) * (1 - validation_split))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    input_dim = 28 * 28\n",
        "    model = MNISTSDEModel(data_dim=input_dim)\n",
        "\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Linear(input_dim, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10)\n",
        "    )\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=learning_rate)\n",
        "\n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}',\n",
        "                          postfix={'train_loss': 0.0, 'test_accuracy': 0.0})\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_pbar):\n",
        "\n",
        "            data = data.view(data.size(0), -1)\n",
        "\n",
        "            batch_size = data.size(0)\n",
        "            t0, t1 = 0.0, 1.0\n",
        "            ts = torch.tensor([t0, t1])\n",
        "            xs = torchsde.sdeint(model, data, ts, dt=0.1)\n",
        "\n",
        "            outputs = classifier(xs[-1])\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            train_pbar.set_postfix({\n",
        "                'train_loss': f'{loss.item():.4f}'\n",
        "            })\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_pbar = tqdm(test_loader, desc='Testing', leave=False)\n",
        "            for data, target in test_pbar:\n",
        "                data = data.view(data.size(0), -1)\n",
        "                xs = model.sample(data.size(0), x0=data, t=t0, dt=0.1)\n",
        "                outputs = classifier(xs[-1])\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "\n",
        "                test_pbar.set_postfix({'accuracy': f'{100 * correct / total:.2f}%'})\n",
        "\n",
        "        test_accuracy = 100 * correct / total\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        train_pbar.close()\n",
        "\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    print(f\"Best Test Accuracy: {max(test_accuracies):.2f}%\")\n",
        "\n",
        "    return model, classifier, train_losses, test_accuracies\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    model, classifier, train_losses, test_accuracies = train_mnist_sde(epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O8P3YtF-BiR",
        "outputId": "7f553e6e-5af9-4e13-a68a-45ef16a2ca96"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 14.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 493kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 235/235 [01:34<00:00,  2.48it/s, train_loss=0.3029]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "  Train Loss: 0.4158\n",
            "  Test Accuracy: 79.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 235/235 [01:31<00:00,  2.56it/s, train_loss=0.1341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:\n",
            "  Train Loss: 0.1839\n",
            "  Test Accuracy: 73.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 235/235 [01:32<00:00,  2.53it/s, train_loss=0.1850]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:\n",
            "  Train Loss: 0.1302\n",
            "  Test Accuracy: 69.64%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 235/235 [02:00<00:00,  1.95it/s, train_loss=0.0815]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4:\n",
            "  Train Loss: 0.1089\n",
            "  Test Accuracy: 69.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 235/235 [01:34<00:00,  2.48it/s, train_loss=0.0733]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5:\n",
            "  Train Loss: 0.0896\n",
            "  Test Accuracy: 65.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 235/235 [01:35<00:00,  2.47it/s, train_loss=0.0703]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6:\n",
            "  Train Loss: 0.0719\n",
            "  Test Accuracy: 61.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 235/235 [01:35<00:00,  2.47it/s, train_loss=0.0497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7:\n",
            "  Train Loss: 0.0617\n",
            "  Test Accuracy: 63.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 235/235 [01:35<00:00,  2.47it/s, train_loss=0.0123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8:\n",
            "  Train Loss: 0.0537\n",
            "  Test Accuracy: 69.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 235/235 [01:34<00:00,  2.48it/s, train_loss=0.0542]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9:\n",
            "  Train Loss: 0.0498\n",
            "  Test Accuracy: 64.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 235/235 [01:34<00:00,  2.50it/s, train_loss=0.0847]\n",
            "                                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10:\n",
            "  Train Loss: 0.0433\n",
            "  Test Accuracy: 63.69%\n",
            "Best Test Accuracy: 79.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}